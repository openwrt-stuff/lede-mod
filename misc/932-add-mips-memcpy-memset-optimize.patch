diff --git a/src/string/memcpy.c b/src/string/memcpy.c
toolchain/musl/patches/932-add-mips-memcpy-memset-optimize.patch
--- a/src/string/memcpy.c	2017-01-03 08:47:12.000000000 +0800
+++ b/src/string/memcpy.c	2017-07-22 12:30:27.000000000 +0800
@@ -1,124 +1,83 @@
+/*
+ * Copyright (C) 2004 Joakim Tjernlund
+ * Copyright (C) 2000-2005 Erik Andersen <andersen@uclibc.org>
+ *
+ * Licensed under the LGPL v2.1, see the file COPYING.LIB in this tarball.
+ */
+
+/* These are carefully optimized mem*() functions for PPC written in C.
+ * Don't muck around with these function without checking the generated
+ * assembler code.
+ * It is possible to optimize these significantly more by using specific
+ * data cache instructions(mainly dcbz). However that requires knownledge
+ * about the CPU's cache line size.
+ *
+ * BUG ALERT!
+ * The cache instructions on MPC8xx CPU's are buggy(they don't update
+ * the DAR register when causing a DTLB Miss/Error) and cannot be
+ * used on 8xx CPU's without a kernel patch to work around this
+ * problem.
+ */
+
 #include <string.h>
 #include <stdint.h>
 #include <endian.h>
 
-void *memcpy(void *restrict dest, const void *restrict src, size_t n)
+/* PPC can do pre increment and load/store, but not post increment and
+   load/store.  Therefore use *++ptr instead of *ptr++.  */
+void *memcpy(void *restrict to, const void *restrict from, size_t len)
 {
-	unsigned char *d = dest;
-	const unsigned char *s = src;
-
-#ifdef __GNUC__
-
-#if __BYTE_ORDER == __LITTLE_ENDIAN
-#define LS >>
-#define RS <<
-#else
-#define LS <<
-#define RS >>
-#endif
-
-	typedef uint32_t __attribute__((__may_alias__)) u32;
-	uint32_t w, x;
-
-	for (; (uintptr_t)s % 4 && n; n--) *d++ = *s++;
-
-	if ((uintptr_t)d % 4 == 0) {
-		for (; n>=16; s+=16, d+=16, n-=16) {
-			*(u32 *)(d+0) = *(u32 *)(s+0);
-			*(u32 *)(d+4) = *(u32 *)(s+4);
-			*(u32 *)(d+8) = *(u32 *)(s+8);
-			*(u32 *)(d+12) = *(u32 *)(s+12);
-		}
-		if (n&8) {
-			*(u32 *)(d+0) = *(u32 *)(s+0);
-			*(u32 *)(d+4) = *(u32 *)(s+4);
-			d += 8; s += 8;
-		}
-		if (n&4) {
-			*(u32 *)(d+0) = *(u32 *)(s+0);
-			d += 4; s += 4;
-		}
-		if (n&2) {
-			*d++ = *s++; *d++ = *s++;
-		}
-		if (n&1) {
-			*d = *s;
-		}
-		return dest;
-	}
-
-	if (n >= 32) switch ((uintptr_t)d % 4) {
-	case 1:
-		w = *(u32 *)s;
-		*d++ = *s++;
-		*d++ = *s++;
-		*d++ = *s++;
-		n -= 3;
-		for (; n>=17; s+=16, d+=16, n-=16) {
-			x = *(u32 *)(s+1);
-			*(u32 *)(d+0) = (w LS 24) | (x RS 8);
-			w = *(u32 *)(s+5);
-			*(u32 *)(d+4) = (x LS 24) | (w RS 8);
-			x = *(u32 *)(s+9);
-			*(u32 *)(d+8) = (w LS 24) | (x RS 8);
-			w = *(u32 *)(s+13);
-			*(u32 *)(d+12) = (x LS 24) | (w RS 8);
-		}
-		break;
-	case 2:
-		w = *(u32 *)s;
-		*d++ = *s++;
-		*d++ = *s++;
-		n -= 2;
-		for (; n>=18; s+=16, d+=16, n-=16) {
-			x = *(u32 *)(s+2);
-			*(u32 *)(d+0) = (w LS 16) | (x RS 16);
-			w = *(u32 *)(s+6);
-			*(u32 *)(d+4) = (x LS 16) | (w RS 16);
-			x = *(u32 *)(s+10);
-			*(u32 *)(d+8) = (w LS 16) | (x RS 16);
-			w = *(u32 *)(s+14);
-			*(u32 *)(d+12) = (x LS 16) | (w RS 16);
-		}
-		break;
-	case 3:
-		w = *(u32 *)s;
-		*d++ = *s++;
-		n -= 1;
-		for (; n>=19; s+=16, d+=16, n-=16) {
-			x = *(u32 *)(s+3);
-			*(u32 *)(d+0) = (w LS 8) | (x RS 24);
-			w = *(u32 *)(s+7);
-			*(u32 *)(d+4) = (x LS 8) | (w RS 24);
-			x = *(u32 *)(s+11);
-			*(u32 *)(d+8) = (w LS 8) | (x RS 24);
-			w = *(u32 *)(s+15);
-			*(u32 *)(d+12) = (x LS 8) | (w RS 24);
-		}
-		break;
-	}
-	if (n&16) {
-		*d++ = *s++; *d++ = *s++; *d++ = *s++; *d++ = *s++;
-		*d++ = *s++; *d++ = *s++; *d++ = *s++; *d++ = *s++;
-		*d++ = *s++; *d++ = *s++; *d++ = *s++; *d++ = *s++;
-		*d++ = *s++; *d++ = *s++; *d++ = *s++; *d++ = *s++;
-	}
-	if (n&8) {
-		*d++ = *s++; *d++ = *s++; *d++ = *s++; *d++ = *s++;
-		*d++ = *s++; *d++ = *s++; *d++ = *s++; *d++ = *s++;
-	}
-	if (n&4) {
-		*d++ = *s++; *d++ = *s++; *d++ = *s++; *d++ = *s++;
-	}
-	if (n&2) {
-		*d++ = *s++; *d++ = *s++;
-	}
-	if (n&1) {
-		*d = *s;
-	}
-	return dest;
-#endif
-
-	for (; n; n--) *d++ = *s++;
-	return dest;
+	unsigned long rem, chunks, tmp1, tmp2;
+	unsigned char *tmp_to = to;
+	const unsigned char *tmp_from = from;
+
+	chunks = len / 8;
+	tmp_from -= 4;
+	tmp_to -= 4;
+	if (!chunks)
+		goto lessthan8;
+	rem = (unsigned long )tmp_to % 4;
+	if (rem)
+		goto align;
+ copy_chunks:
+	do {
+		/* make gcc to load all data, then store it */
+		tmp1 = *(unsigned long *)(tmp_from+4);
+		tmp_from += 8;
+		tmp2 = *(unsigned long *)tmp_from;
+		*(unsigned long *)(tmp_to+4) = tmp1;
+		tmp_to += 8;
+		*(unsigned long *)tmp_to = tmp2;
+	} while (--chunks);
+ lessthan8:
+	len = len % 8;
+	if (len >= 4) {
+		tmp_from += 4;
+		tmp_to += 4;
+		*(unsigned long *)(tmp_to) = *(unsigned long *)(tmp_from);
+		len -= 4;
+	}
+	if (!len)
+		return to;
+	tmp_from += 3;
+	tmp_to += 3;
+	do {
+		*++tmp_to = *++tmp_from;
+	} while (--len);
+
+	return to;
+ align:
+	/* ???: Do we really need to generate the carry flag here? If not, then:
+	rem -= 4; */
+	rem = 4 - rem;
+	len -= rem;
+	do {
+		*(tmp_to+4) = *(tmp_from+4);
+		++tmp_from;
+		++tmp_to;
+	} while (--rem);
+	chunks = len / 8;
+	if (chunks)
+		goto copy_chunks;
+	goto lessthan8;
 }
--- a/src/string/memmove.c	2017-07-22 09:51:22.066177025 +0800
+++ b/src/string/memmove.c	2017-07-20 19:56:14.000000000 +0800
@@ -1,36 +1,78 @@
+/*
+ * Copyright (C) 2004 Joakim Tjernlund
+ * Copyright (C) 2000-2005 Erik Andersen <andersen@uclibc.org>
+ *
+ * Licensed under the LGPL v2.1, see the file COPYING.LIB in this tarball.
+ */
+
+/* These are carefully optimized mem*() functions for PPC written in C.
+ * Don't muck around with these function without checking the generated
+ * assembler code.
+ * It is possible to optimize these significantly more by using specific
+ * data cache instructions(mainly dcbz). However that requires knownledge
+ * about the CPU's cache line size.
+ *
+ * BUG ALERT!
+ * The cache instructions on MPC8xx CPU's are buggy(they don't update
+ * the DAR register when causing a DTLB Miss/Error) and cannot be
+ * used on 8xx CPU's without a kernel patch to work around this
+ * problem.
+ */
+
 #include <string.h>
 #include <stdint.h>
 
 #define WT size_t
 #define WS (sizeof(WT))
 
-void *memmove(void *dest, const void *src, size_t n)
+void *memmove(void *to, const void *from, size_t n)
 {
-	char *d = dest;
-	const char *s = src;
-
-	if (d==s) return d;
-	if (s+n <= d || d+n <= s) return memcpy(d, s, n);
+	unsigned long rem, chunks, tmp1, tmp2;
+	unsigned char *tmp_to = to;
+	unsigned char *tmp_from = (unsigned char *)from;
 
-	if (d<s) {
-		if ((uintptr_t)s % WS == (uintptr_t)d % WS) {
-			while ((uintptr_t)d % WS) {
-				if (!n--) return dest;
-				*d++ = *s++;
-			}
-			for (; n>=WS; n-=WS, d+=WS, s+=WS) *(WT *)d = *(WT *)s;
-		}
-		for (; n; n--) *d++ = *s++;
-	} else {
-		if ((uintptr_t)s % WS == (uintptr_t)d % WS) {
-			while ((uintptr_t)(d+n) % WS) {
-				if (!n--) return dest;
-				d[n] = s[n];
-			}
-			while (n>=WS) n-=WS, *(WT *)(d+n) = *(WT *)(s+n);
-		}
-		while (n) n--, d[n] = s[n];
+	if (tmp_from >= (unsigned char *)to)
+		return memcpy(to, from, n);
+	chunks = n / 8;
+	tmp_from += n;
+	tmp_to += n;
+	if (!chunks)
+		goto lessthan8;
+	rem = (unsigned long )tmp_to % 4;
+	if (rem)
+		goto align;
+ copy_chunks:
+	do {
+		/* make gcc to load all data, then store it */
+		tmp1 = *(unsigned long *)(tmp_from-4);
+		tmp_from -= 8;
+		tmp2 = *(unsigned long *)tmp_from;
+		*(unsigned long *)(tmp_to-4) = tmp1;
+		tmp_to -= 8;
+		*(unsigned long *)tmp_to = tmp2;
+	} while (--chunks);
+ lessthan8:
+	n = n % 8;
+	if (n >= 4) {
+		*(unsigned long *)(tmp_to-4) = *(unsigned long *)(tmp_from-4);
+		tmp_from -= 4;
+		tmp_to -= 4;
+		n = n-4;
 	}
+	if (!n ) return to;
+	do {
+		*--tmp_to = *--tmp_from;
+	} while (--n);
 
-	return dest;
+	return to;
+ align:
+	rem = 4 - rem;
+	n = n - rem;
+	do {
+		*--tmp_to = *--tmp_from;
+	} while (--rem);
+	chunks = n / 8;
+	if (chunks)
+		goto copy_chunks;
+	goto lessthan8;
 }
--- a/src/string/memset.c	2017-07-22 09:51:22.066177025 +0800
+++ b/src/string/memset.c	2017-07-20 19:56:14.000000000 +0800
@@ -1,86 +1,81 @@
+/*
+ * Copyright (C) 2004 Joakim Tjernlund
+ * Copyright (C) 2000-2005 Erik Andersen <andersen@uclibc.org>
+ *
+ * Licensed under the LGPL v2.1, see the file COPYING.LIB in this tarball.
+ */
+
+/* These are carefully optimized mem*() functions for PPC written in C.
+ * Don't muck around with these function without checking the generated
+ * assembler code.
+ * It is possible to optimize these significantly more by using specific
+ * data cache instructions(mainly dcbz). However that requires knownledge
+ * about the CPU's cache line size.
+ *
+ * BUG ALERT!
+ * The cache instructions on MPC8xx CPU's are buggy(they don't update
+ * the DAR register when causing a DTLB Miss/Error) and cannot be
+ * used on 8xx CPU's without a kernel patch to work around this
+ * problem.
+ */
+
 #include <string.h>
 #include <stdint.h>
 
-void *memset(void *dest, int c, size_t n)
+static __inline__ int expand_byte_word(int c){
+	/* this does:
+	   c = c << 8 | c;
+	   c = c << 16 | c ;
+	*/
+	__asm__("rlwimi	%0,%0,8,16,23\n"
+	    "\trlwimi	%0,%0,16,0,15\n"
+	    : "=r" (c) : "0" (c));
+	return c;
+}
+
+void *memset(void *to, int c, size_t n)
 {
-	unsigned char *s = dest;
-	size_t k;
+	unsigned long rem, chunks;
+	unsigned char *tmp_to = to;
 
-	/* Fill head and tail with minimal branching. Each
-	 * conditional ensures that all the subsequently used
-	 * offsets are well-defined and in the dest region. */
-
-	if (!n) return dest;
-	s[0] = s[n-1] = c;
-	if (n <= 2) return dest;
-	s[1] = s[n-2] = c;
-	s[2] = s[n-3] = c;
-	if (n <= 6) return dest;
-	s[3] = s[n-4] = c;
-	if (n <= 8) return dest;
-
-	/* Advance pointer to align it at a 4-byte boundary,
-	 * and truncate n to a multiple of 4. The previous code
-	 * already took care of any head/tail that get cut off
-	 * by the alignment. */
-
-	k = -(uintptr_t)s & 3;
-	s += k;
-	n -= k;
-	n &= -4;
-
-#ifdef __GNUC__
-	typedef uint32_t __attribute__((__may_alias__)) u32;
-	typedef uint64_t __attribute__((__may_alias__)) u64;
-
-	u32 c32 = ((u32)-1)/255 * (unsigned char)c;
-
-	/* In preparation to copy 32 bytes at a time, aligned on
-	 * an 8-byte bounary, fill head/tail up to 28 bytes each.
-	 * As in the initial byte-based head/tail fill, each
-	 * conditional below ensures that the subsequent offsets
-	 * are valid (e.g. !(n<=24) implies n>=28). */
-
-	*(u32 *)(s+0) = c32;
-	*(u32 *)(s+n-4) = c32;
-	if (n <= 8) return dest;
-	*(u32 *)(s+4) = c32;
-	*(u32 *)(s+8) = c32;
-	*(u32 *)(s+n-12) = c32;
-	*(u32 *)(s+n-8) = c32;
-	if (n <= 24) return dest;
-	*(u32 *)(s+12) = c32;
-	*(u32 *)(s+16) = c32;
-	*(u32 *)(s+20) = c32;
-	*(u32 *)(s+24) = c32;
-	*(u32 *)(s+n-28) = c32;
-	*(u32 *)(s+n-24) = c32;
-	*(u32 *)(s+n-20) = c32;
-	*(u32 *)(s+n-16) = c32;
-
-	/* Align to a multiple of 8 so we can fill 64 bits at a time,
-	 * and avoid writing the same bytes twice as much as is
-	 * practical without introducing additional branching. */
-
-	k = 24 + ((uintptr_t)s & 4);
-	s += k;
-	n -= k;
-
-	/* If this loop is reached, 28 tail bytes have already been
-	 * filled, so any remainder when n drops below 32 can be
-	 * safely ignored. */
-
-	u64 c64 = c32 | ((u64)c32 << 32);
-	for (; n >= 32; n-=32, s+=32) {
-		*(u64 *)(s+0) = c64;
-		*(u64 *)(s+8) = c64;
-		*(u64 *)(s+16) = c64;
-		*(u64 *)(s+24) = c64;
+	chunks = n / 8;
+	tmp_to -= 4;
+	c = expand_byte_word(c);
+	if (!chunks)
+		goto lessthan8;
+	rem = (unsigned long )tmp_to % 4;
+	if (rem)
+		goto align;
+ copy_chunks:
+	do {
+		*(unsigned long *)(tmp_to+4) = c;
+		tmp_to += 4;
+		*(unsigned long *)(tmp_to+4) = c;
+		tmp_to += 4;
+	} while (--chunks);
+ lessthan8:
+	n = n % 8;
+	if (n >= 4) {
+		*(unsigned long *)(tmp_to+4) = c;
+		tmp_to += 4;
+		n = n-4;
 	}
-#else
-	/* Pure C fallback with no aliasing violations. */
-	for (; n; n--, s++) *s = c;
-#endif
-
-	return dest;
+	if (!n ) return to;
+	tmp_to += 3;
+	do {
+		*++tmp_to = c;
+	} while (--n);
+
+	return to;
+ align:
+	rem = 4 - rem;
+	n = n-rem;
+	do {
+		*(tmp_to+4) = c;
+		++tmp_to;
+	} while (--rem);
+	chunks = n / 8;
+	if (chunks)
+		goto copy_chunks;
+	goto lessthan8;
 }
